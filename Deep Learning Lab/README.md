This information has been fetched from the university's system.

# Item description:

In the lab, we will understand the operation of the most powerful generative models currently available, i.e. transformers and diffusion models. We will also learn about GANs, variational autoencoders, recurrent neural networks, and the attention mechanism itself. The means to achieve this will be to learn the absolute classics of deep learning, such as Kohonen Maps, as well as hidden treasures such as UMAP. A side effect will be that each of us will finally understand how t-SNE works.

In each class, we will develop a very simplified architecture of a given model to solve simple generative problems. And then, as part of the homework, we will implement it together.

The datasets we will use are mainly MNIST and a self-written context-free grammar generator.

The educational goals at the end of this class are:

* to be able to hand-code each of these models from scratch
* to fully understand all the intuitions and mathematical motivations behind the architecture of these models

The course is graded based on points collected from programming homework assignments.

The final project can be reached (called EchoesOfFame) [Project](https://github.com/berayboztepe/EchoesOfFame)
